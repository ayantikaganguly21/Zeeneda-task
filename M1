Session01_Zeneeda:
Question 1: What was the objective of the Session 1 Hands-on Lab?
Answer: The objective of the Session 1 hands-on lab was to build a functional Notion-based Analyst OS workspace that integrates AI into the analysis workflow in a structured and responsible manner. The goal was to create a repeatable system that transforms raw stakeholder inputs into reviewable and traceable deliverables. By the end of the lab, the workspace was expected to include structured databases, meeting documentation, requirement candidates, decisions, risks, and an AI usage log. The lab emphasized combining AI acceleration with human validation to ensure quality, transparency, and accountability.
Question 2: What workspace structure was created in Notion during the lab?
Answer: During the lab, a structured workspace was created with top-level pages including Start Here, Stakeholders, Meetings & Notes, Requirements Candidates, Decisions & Assumptions, Risks & Issues, and AI Use Log. Each page served a specific purpose to maintain organization and traceability. The “Start Here” page defined rules for AI usage, naming conventions, and data handling guidelines. The databases were created as full-page databases with core properties such as date, role, owner, status, severity, and source links to maintain structured documentation.
Question 3: How was the meeting transcript processed using AI?
Answer: The given meeting transcript about support ticket issues was entered into the Meetings & Notes database. AI was then used to generate a structured summary highlighting the key problems and stakeholder needs. It extracted action items such as creating a unified intake form, enabling automatic assignment based on category, generating weekly SLA reports, supporting attachments, and maintaining audit history. The AI also helped identify requirement candidates. However, all AI outputs were carefully reviewed and validated by applying human judgment before being finalized and stored in the respective databases.
Question 4: What requirement candidates were identified from the meeting?
Answer: Five key requirement candidates were identified from the meeting discussion. These included the need for a single ticket intake form with priority, category, and requester details; automatic ticket assignment based on category; weekly reporting of backlog by priority, average response time, and SLA breaches; support for file attachments; and maintenance of an audit history of changes. Each candidate was categorized by type, linked to the source meeting, assigned a confidence level, and marked with status for further validation.
Question 5: How were quality gates applied in the lab?
Answer: Quality gates were applied by selecting two requirement candidates and verifying them against three checks: evidence, correctness, and stakeholder alignment. Evidence ensured that each requirement traced back to the meeting transcript. Correctness ensured that the interpretation was accurate and not based on assumptions. Alignment ensured that stakeholders agreed with the interpretation and scope. This process ensured that AI-generated drafts were transformed into validated and reliable artifacts.
Question 6: What decision, risk, and AI log entry were recorded?
Answer: One formal decision was recorded stating that Phase 1 implementation would include only the top five categories, while additional categories would be considered in Phase 2. One risk was identified that SLA rules were ambiguous and required further clarification to avoid misinterpretation during development. Additionally, an AI Use Log entry was created documenting how AI was used for summarization and requirement extraction, what outputs were generated, what verification steps were taken, and confirming that human validation was performed before sharing the final artifacts.

